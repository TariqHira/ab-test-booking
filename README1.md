# ‚úàÔ∏è A/B Testing Flight Booking Conversion Rates

> üåü A practical data science project simulating an A/B test to evaluate the effect of a new pricing strategy on flight booking conversions ‚Äî set in the context of an imaginary travel platform, **MyTravel.com**.

---

## üìå Project Overview

The Flight Booking Team at MyTravel.com is planning to roll out a new pricing strategy and wants to test its impact before a full launch. To assess whether this strategy is actually increasing flight bookings ‚Äî or if the observed change is simply due to chance ‚Äî they've tasked **Miss Reem**, a **Data Scientist**, with designing and conducting a hypothesis test.

Miss Reem's job isn't just to crunch numbers; it's to ensure the experiment is statistically and practically significant, the data is analyzed rigorously, and the results are interpreted in a way that guides strategic decisions.

This project takes you through a realistic workflow:

- Simulate user-level booking data
- Design and implement an A/B test
- Apply hypothesis testing
- Visualize and interpret results

All with the goal of helping data professionals build intuition and confidence in experimentation.

---

## üë• Target Audience

This project is designed for:

- Aspiring or working **Data Scientists**, **Analysts**, and **Product Managers**
- Candidates preparing for **A/B testing or product analytics interviews**
- Learners looking to **understand and apply hypothesis testing in a practical setting**

---

## üß™ Why Hypothesis Testing?

The key question in business often is:

> "Is the observed improvement real or just random noise?"

In business, we frequently want to validate if a new strategy truly creates an improvement. But natural variation makes this tricky ‚Äî especially for proportions (e.g., booking conversion rates).

That‚Äôs where **hypothesis testing** comes in.

---

### üîç What is Hypothesis Testing?

Miss Reem knows that it's impossible and impractical to run the experiment on the entire population. Instead, she can run an experiment on selected samples from the population. She's also well aware of the statistical method "hypothesis testing," which is used to make decisions or inferences about population parameters based on sample data. The process involves:

1. **Formulating two hypotheses:**

   - **Null Hypothesis (H‚ÇÄ):** Assumes no effect or no difference (e.g., "There is no difference in conversion rates between the control and treatment pages").
   - **Alternative Hypothesis (H‚ÇÅ):** Assumes there is an effect or a difference (e.g., "The new pricing page increases conversions").

2. **Designing the experiment:** Estimating the sample size based on Minimum Detectable Effect (MDE), Power, and Significance.

3. **Collecting data** from experiments or observations.

4. **Analyzing the data** to determine whether the evidence is strong enough to reject the null hypothesis in favor of the alternative.

---

### üß™ What is A/B Experimentation?

Miss Reem decides to implement an A/B test ‚Äî a practical form of hypothesis testing. It compares two versions (A and B) of a feature by randomly splitting users and measuring which performs better on a defined metric (like conversion rate).

---

## üìä Two-Proportion Hypothesis Testing (A/B Experiment)

Two-proportion hypothesis testing is used to determine if there is a statistically significant difference between the proportions of a certain characteristic (e.g., conversion rate) in two independent groups.

**When to Use It:**

- You have two independent samples (e.g., control vs. treatment).
- Each sample yields a proportion (e.g., % of users who book flights).
- You want to know if the difference in these proportions is due to chance.

| Group | Description     |
| ----- | --------------- |
| A     | Control Group   |
| B     | Treatment Group |

**Null Hypothesis (H‚ÇÄ):** pA ‚â§ pB\
**Alternative Hypothesis (H‚ÇÅ):** pA > pB (one-tailed)

---

## üåü One-Tailed vs. Two-Tailed Tests

The choice of a one-tailed or two-tailed alternative hypothesis is critical.

- **One-Tailed Test (H‚ÇÅ: p‚Çí > p‚Çë):** This is used when you are only interested in detecting an effect in a specific direction. For example, if the new pricing strategy is only considered successful if it *increases* conversions, a one-tailed test is appropriate. It offers more statistical power to detect an effect in the specified direction. This means the rejection region for the null hypothesis is entirely in one tail of the sampling distribution.

- **Two-Tailed Test (H‚ÇÅ: p‚Çí ‚â† p‚Çë):** This is more common in exploratory A/B testing as it can detect a significant difference in *either* direction (increase or decrease). If you're unsure whether the new strategy might increase or decrease conversions, or if a decrease would also be an important finding, a two-tailed test is more suitable. The rejection region is split between both tails of the sampling distribution.

In this project, a one-tailed test is chosen, implying that the team is primarily interested in detecting a positive uplift in conversion rates.

---

## üìú Assumptions for Two-Proportion Z-Test

To ensure valid results, the following assumptions must hold:

- **Independence of observations** The outcomes for one user should not influence the outcomes for another user¬†(via random assignment)
- **Independent samples** (no overlap between control and treatment)
- **Large enough sample sizes**:¬†For the sampling distribution of the proportions to be approximately normal (Law of Large Numbers) - allowing the use of Z-scores - a common rule of thumb is at least 10 in each category

---

## üß™ Designing the Experiment

Miss Reem carefully considers the following:

### ‚úÖ A/B Test Design Checklist

- **Define Business Impact (Minimum Detectable Effect - MDE):**

  - What uplift (e.g., +5% conversion) is meaningful to detect? The MDE is not just a statistical parameter; it's a **business decision**. It represents the smallest change that would be considered financially or strategically worthwhile to implement. For instance, a 0.5% conversion uplift might be statistically significant but not justify the development and maintenance costs.
  - **Calculating MDE:** If your baseline conversion rate (p1‚Äã) is known, and you define the MDE as a percentage *uplift* of that baseline, you can calculate p2‚Äã.
    - If the MDE is given as an *absolute* difference (e.g., an absolute increase of 5 percentage points), then p2‚Äã=p1‚Äã+5% --> if p1 is 10 then p2 = 15% = 0.15
    - If the MDE is given as a¬†*relative*¬†uplift of let's say 5% and p1 is 10% then p2 = 0.105 =10.5%¬†\


- **Estimate Baseline (**************************************************************************************************************************************************************************************************************************************************************************************************************************p\_1**************************************************************************************************************************************************************************************************************************************************************************************************************************):**

  - Use historical data (last campaign, last month, etc.) to get an accurate estimate of the current conversion rate.

- **Choose Œ±**¬†

  Confidence Interval ‚Äî A **confidence interval** is a range of values that we're confident contains the **true effect** of what we're testing. The confidence level (e.g., 95%) indicates how likely it is that the interval contains the true population parameter. The significance level Œ± is the complement of the confidence level (Œ± = 1 ‚àí CI). For example, a 95% confidence level corresponds to Œ± = 0.05. This Œ± value represents the probability of making a **Type I error** ‚Äî falsely rejecting the null hypothesis when it is actually true. that we‚Äôre confident contains the **true effect** of what we‚Äôre testing.¬†**Œ±**¬†is (1- CI)

- **Choose Power**¬†

  Rejecting Null Hypothesis when it's False is power (here it's like rejecting that the conversion rate increased for the Treatment when it actually holds true for the entire population)

The goal is either to **Reject the Null Hypothesis** if it's not true (related to the **Power** of a Test) or **Do Not Reject** it if it is, in fact, true, based on the test results. If the test is poorly designed, the results can be biased and lead to the following errors.

## ‚öñÔ∏è Type I and Type II Errors

| Error Type | Description                                        | Controlled by        | Example                        |
| ---------- | -------------------------------------------------- | -------------------- | ------------------------------ |
| Type I     | False positive ‚Äî reject H‚ÇÄ when it‚Äôs true          | Significance level Œ± | You say B > A, but it‚Äôs not    |
| Type II    | False negative ‚Äî fail to reject H‚ÇÄ when it‚Äôs false | Power = 1 ‚àí Œ≤        | You say B = A, but B is better |

###

- **Calculate Minimum Sample Size:**

  Either through pooled statistics or through Cohen's d, Cohen's h

  **Sample size using raw proportions**
  **Formula (per group):**
  \
  \(n = \frac{\left(Z_{1 - \alpha/2} \cdot \sqrt{2p(1 - p)} + Z_{1 - \beta} \cdot \sqrt{p_1(1 - p_1) + p_2(1 - p_2)}\right)^2}{(p_1 - p_2)^2}\)
    
    Where:
    \(p = \frac{p_1 + p_2}{2}\)

  **Sample size using Cohen‚Äôs h (standardized effect size)**
  \
  \(n = \frac{h^2}{2} \cdot \left(Z_{1 - \alpha/2} + Z_{1 - \beta}\right)^2\)

### why we choose one formula over the other
When historical data exists ‚Üí use raw proportions
When only effect size is known (e.g., small, medium, large), use Cohen‚Äôs h


- Why Cohen's Tranformation?¬†\
  A **5% absolute difference** in proportions can have **very different magnitudes of impact** depending on **where it lies on the probability scale**. That's **exactly why Cohen's h** was created ‚Äî to **standardize** the effect size on a **non-linear probability scale**.

## Why a 5% Difference Isn't Always Equal

Let‚Äôs look at two examples:

|     Comparison       |  p1  |  p2  | Absolute Difference |Cohen‚Äôs h (Standardized)|
| ---------------------| ---- | ---- | --------------------| -----------------------|
|   Small Baseline     | 0.10 | 0.15 |     0.05 (5%)       |          ‚âà 0.148       |
|   Large Baseline     | 0.85 | 0.90 |     0.05 (5%)       |          ‚âà 0.105       |

‚û°Ô∏è Although the **absolute difference** is 5% in both cases, the **Cohen‚Äôs h** is **smaller** for the high baseline. Why?

### üî¨ Because of the shape of the probability distribution:

- A difference from 10% to 15% means a **50% relative increase** in conversion.
- A difference from 85% to 90% is only **5.9% relative increase**.
- Also, **variance is not uniform** across the 0‚Äì1 probability scale.

> This **non-linearity** is captured by the **arcsin square root transformation** in Cohen's h.

---

## üìä Understanding Effect Size: Cohen's d and Cohen's h

- **Cohen's d (for continuous data):**

  **When to use:** Cohen's d is used when comparing the means of two groups on a **continuous variable** (e.g., average revenue per user, time spent on page). It expresses the difference between two means in terms of standard deviation units.

  \
  \(d = \frac{\bar{x}_1 - \bar{x}_2}{s_p}\)



- **Cohen's h (for proportions):**

  **When to use:** Cohen's h is specifically designed for comparing **two proportions**, which is highly relevant for A/B testing conversion rates. It's a measure of effect size that accounts for the non-linear nature of proportions by using an arcsine transformation.

  \
  \(h = 2 \cdot \left( \arcsin(\sqrt{p_1}) - \arcsin(\sqrt{p_2}) \right)\)

**Interpretation (both):**

- 0.2: Small effect
- 0.5: Medium effect
- 0.8: Large effect\

  ## üìä 4. Relationship Between CI, Type I Error, and Power
  |             Concept                    |            What it tells              |          Controlled by          |
  | -------------------------------------- | ------------------------------------- | ------------------------------- |
  | **Confidence Interval**                | The range of the possible true effect | Sample size, variability, alpha |
  | **Type I Error (Œ±)**                   | Risk of a **false positive**          | Significance level (alpha)      |
  | **Power (1 - Œ≤)**                      | Ability to detect a **true positive** | Sample size, effect size, alpha |

---

### üõ°Ô∏è How to Avoid Errors

- Lower Œ± to reduce Type I error (e.g., 0.01)
- Increase power to reduce Type II error (e.g., 0.90)
- Pre-register experiment design

